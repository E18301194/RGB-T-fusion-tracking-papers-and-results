# RGB-T fusion tracking：papers，results & datasets
The papers and results about RGB-T fusion tracking

## Papers
### Journal papers
- Xiangyuan Lan, Mang Ye, Rui Shan, Bineng Zhong, Deepak Kumar Jain, Huiyu Zhou  
	“Online non-negative multi-modality feature template learning for RGB-assisted infrared tracking”. IEEE Access, 2019. [\[paper]]()
- Chengwei Luo, Bin Sun, Ke Yang, Taoran Lu, Wei-Chang Yeh.  
	  “Thermal infrared and visible sequences fusion tracking based on a hybrid tracking framework with adaptive weighting scheme”. Infrared Physics & Technology, 2019. (only use a part of RGBT234.)
- Sulan Zhai, Pengpeng Shao, Xinyan Liang, Xin Wang.  
	"Fast RGB-T Tracking via Cross-Modal Correlation Filters." Neurocomputing (2019).[[paper][2]]
- Xiangyuan Lan, Mang Ye, Rui Shao, Bineng Zhong, Pong C.Yuen, Huiyu Zhou.  
	  "Learning Modality-Consistency Feature Templates: A Robust RGB-Infrared Tracking System."IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS (2019).[[paper][3]]
- Chenglong Li, Chengli Zhu, Jian Zhang, Bin Luo, Xiaohao Wu, and Jin Tang.  
	  "Learning Local-Global Multi-Graph Descriptors for RGB-T Object Tracking". IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT), (2019).[[paper][4]] (**FPS: 0.7**)
- Xiangyuan Lan, Mang Ye, Shengping Zhang, Huiyu Zhou, Pong C. Yuen.  
	  "Modality-correlation-aware sparse representation for RGB-infrared object tracking".Pattern Recognition Letters, 2019
- Keyan Ren, Xiao Zhang, Yu Han, Yibin Hou.  
	   "Robust night target tracking via infrared and visible video fusion". Applications of Digital Image Processing, 2018. (**asynchronous VI and IR videos**)
- Chenglong Li, Xiaohao Wu, Nan Zhao, Xiaochun Cao, and Jin Tang.  
	  "Fusing Two-Stream Convolutional Neural Networks for RGB-T Object Tracking". Neurocomputing (NEUCOM), 281: 78-85, 2018.[[paper][5]]
- Chenglong Li, Chengli Zhu, Shaofei Zheng, Bin Luo, and Jin Tang.  
	  "Two-Stage Modality-Graphs Regularized Manifold Ranking for RGB-T Tracking". Signal Processing: Image Communication (SPIC), 68: 207-217, 2018. [[paper][6]]
- Chenglong Li, Xiang Sun, Xiao Wang, Lei Zhang, and Jin Tang.  
	  "Grayscale-thermal Object Tracking via Multi-task Laplacian Sparse Representation". IEEE Transactions on Systems, Man, and Cybernetics: Systems (T-SMCS), 47(4): 673-681, 2017.[[paper][7]]
- Meng Ding, Yao Yuheng, Li Wei, Yunfeng Cao.  
	  "Visual tracking using Locality-constrained Linear Coding and saliency map for visible light and infrared image sequences". Signal Processing: Image Communication, 2018.
- Chenglong Li, Hui Cheng, Shiyi Hu, Xiaobai Liu, Jin Tang, Liang Lin.  
	    “Learning Collaborative Sparse Representation for Grayscale-Thermal Tracking”,  IEEE Transactions on Image Processing (T-IP), 25(12): 5743-5756, 2016. [[paper][8]]
- Xiao Yun, Zhongliang Jing, Bo Jin.
 "Visible and infrared tracking based on multi-view multi-kernel fusion model". Optical Review, 2016.
- Xiao YUN, Zhongliang JING, Gang XIAO, Bo JIN, Canlong ZHANG.  
	 "A compressive tracking based on time-space Kalman fusion model". Science China Information Sciences, 2016.
- Supriya Mangale, Madhuri Khambete.  
	 "Camouflaged target detection and tracking using thermal infrared and visible spectrum imaging". Advances in Intelligent Systems and Computing, 2016.
- Alex Lipchen Chan, Stephen R. Schnelle.  
	 "Fusing concurrent visible and infrared videos for improved tracking performance". Optical Engineering, 2013.
- Huaping Liu, Fuchun Sun.  
	 "Fusion tracking in color and infrared images using joint sparse representation". Science China Information Sciences, 2012.
- Alex Lipchen Chan, Stephen R. Schnelle.  
	 "Target tracking using concurrent visible and infrared imageries". SPIE, 2012.

### Conference papers
- Xingming Zhang, Xuehan Zhang, Xuedan Du, Xiangming Zhou, Jun Yin.  
	 "Learning Multi-domain Convolutional Network for RGB-T Visual Tracking." CISP-BMEI 2018.
- Chenglong Li, Chengli Zhu, Yan Huang, Jin Tang, Liang Wang.  
	   "Cross-Modal Ranking with Soft Consistency and Noisy Labels for Robust RGB-T Tracking." ECCV 2018.  
- Xiangyuan Lan, Mang Ye, Shengping Zhang, Pong C. Yuen.  
	   "Robust Collaborative Discriminative Learning for RGB-Infrared Tracking". AAAI 2018.
- Yulong Wang, Chenglong Li, and Jin Tang.  
	“Learning Soft-Consistent Correlation Filters for RGB-T Object Tracking”. PRCV 2018.
- Ningwen Xu, Gang Xiao, Xingchen Zhang, Durga Prasad Bavirisetti.  
	"Relative Object Tracking Algorithm Based on Convolutional Neural Network for Visible and Infrared Video Sequences". 4th International Conference on Virtual Reality, 2018
- Ningwen Xu, Gang Xiao, Fang He, Xingchen Zhang, Durga Prasad Bavirisetti.  
	"Object Tracking via Deep Multi-View Compressive Model for Visible and Infrared Sequences". Fusion 2018.
- Chenglong Li, Nan Zhao, Yijuan Lu, Chengli Zhu, and Jin Tang.  
	   "Weighted Sparse Representation Regularized Graph Learning for RGB-T Object Tracking". ACM International Conference on Multimedia (ACM MM), 2017.
- Chenglong Li, Shiyi Hu, Sihan Gao, and Jin Tang.  
	   "Real-time Grayscale-thermal Tracking  via Laplacian Sparse Representation". International Conference on Multimedia Modelling (MMM), Miami, 2016.
- Chengwei Luo, Bin Sun, Qiao Deng, Zihao Wang, Dengwei Wang.  
	    "Comparison of Different Level Fusion Schemes for Infrared-Visible Object Tracking: An Experimental Survey". 2018 2nd International Conference on Robotics and Automation Sciences.
- K. Senthil Kumar1, G. Kavitha, R. Subramanian, G. Ramesh.  
	    "Visual and Thermal Image Fusion of UAV Based Target Tracking". Intech open, 2018.
- Erhan Gundogdu, Huseyin Ozkan, H. Seckin Demir, Hamza Ergezer, Erdem Akag¨und¨uz, S. Kubilay Pakin.  
	 "Comparison of infrared and visible imagery for object tracking: Toward trackers with superior IR performance". CVPR 2015.
- Stephen R. Schnelle, Alex Lipchen Chan.  
	 "Enhanced target tracking through infrared-visible image fusion". Fusion 2011.
- Huaping Liu, Fuchun Sun.  
- "Fusion tracking in color and infrared images using sequential belief propagation". Proceedings - IEEE International Conference on Robotics and Automation, 2008.
### ArXiv
- Chenglong Li, Xinyan Liang, Yijuan Lu, Nan Zhao, and Jin Tang.  
	“RGB-T Object Tracking:Benchmark and Baseline”, 2018
- Yabin Zhu, Chenglong Li, Yijuan Lu, Liang Lin, Bin Luo, Jin Tang.  
	“FANet : Quality-Aware Feature Aggregation Network for RGB-T Tracking”, 2018

### Multispectral person detection (may give some idea to fusion tracking)
- Daniel K¨onig, Michael Adam, Christian Jarvers, Georg Layher, Heiko Neumann, and Michael Teutsch.  
	 "Fully Convolutional Region Proposal Networks for Multispectral Person Detection". CVPR 2017.


## Datasets and benchmark
- GTOT
	- Paper: Chenglong Li, Hui Cheng, Shiyi Hu, Xiaobai Liu, Jin Tang, Liang Lin.  
		“Learning Collaborative Sparse Representation for Grayscale-Thermal Tracking”,  IEEE Transactions on Image Processing (T-IP), 25(12): 5743-5756, 2016. [[paper][9]]
	- Download Link [[Google drive][10]] [[Baidu Cloud][11]]
	- Papers using this dataset
		- Yabin Zhu, Chenglong Li, Yijuan Lu, Liang Lin, Bin Luo, Jin Tang.  
			“FANet : Quality-Aware Feature Aggregation Network for RGB-T Tracking”, 2018 (**PR/SR: 88.5/69.8**)
		- Chenglong Li, Xiaohao Wu, Nan Zhao, Xiaochun Cao, and Jin Tang.  
			  "Fusing Two-Stream Convolutional Neural Networks for RGB-T Object Tracking". Neurocomputing (NEUCOM), 281: 78-85, 2018.[[paper][12]] (**PR/SR:85.2/62.6. Threshold is 5 pixels**)
		- Yulong Wang, Chenglong Li, and Jin Tang.  
			“Learning Soft-Consistent Correlation Filters for RGB-T Object Tracking”. PRCV 2018.(**PR/SR: 85/68.1**)
		- Sulan Zhai, Pengpeng Shao, Xinyan Liang, Xin Wang.  
			   "Fast RGB-T Tracking via Cross-Modal Correlation Filters." Neurocomputing (2019).[[paper][13]] (**PR/SR:77/63.2**)
	- Papers using a part of this dataset
		- Xiangyuan Lan, Mang Ye, Shengping Zhang, Pong C. Yuen.  
			“Robust Collaborative Discriminative Learning for RGB-Infrared Tracking”. AAAI 2018.
		- Xiangyuan Lan, Mang Ye, Rui Shao, Bineng Zhong, Pong C. Yuen, and Huiyu Zhou.  
			“Learning Modality-Consistency Feature Templates: A Robust RGB-Infrared Tracking System”. IEEE Transactions on Industrial Electronics, 2019.
		- Xiangyuan Lan, Mang Ye, Shengping Zhang, Huiyu Zhou, Pong C. Yuen.  
			“Modality-correlation-aware sparse representation for RGB-infrared object tracking”. Pattern Recognition Letters,2018.
- RGBT210
	- Paper: Chenglong Li, Nan Zhao, Yijuan Lu, Chenglin  Zhu, Jin Tang.  
		“Weighted Sparse Representation Regularized Graph Learning for RGB-T Object Tracking”, ACM International Conference on Multimedia (ACM MM), 2017. [[paper][14]]
	- Download Link [[Google drive][15]][[Baidu Cloud][16]]
	- Papers using this dataset
		- **Xingchen Zhang**, Gang Xiao, Ping Ye, Dan Qiao, Shengyun Peng.  
			  "SiamFT: A Feature-level Fusion Tracking Method Using Fully Convolutional Siamese Networks Based on Visible and Infrared Sequences". ICCV 2019 (submitted).(**PR/SR:65.0/44.3**)
		-  Sulan Zhai, Pengpeng Shao, Xinyan Liang, Xin Wang.  
			   "Fast RGB-T Tracking via Cross-Modal Correlation Filters." Neurocomputing (2019).[[paper][17]] (**PR/SR:52.9/36.3**)
		- Chenglong Li, Chengli Zhu, Yan Huang, Jin Tang, Liang Wang.  
			   "Cross-Modal Ranking with Soft Consistency and Noisy Labels for Robust RGB-T Tracking." ECCV 2018. (**PR/SR: 69.4/46.3**) 
- RGBT234
	- Paper: Chenglong Li, Xinyan Liang, Yijuan Lu, Nan Zhao, and Jin Tang.  
		“RGB-T Object Tracking:Benchmark and Baseline”, ArXiv, 2018. Submitted to Pattern Recognition (PR), 2019. [[paper][18]][[project][19]]
	- Download Link [[Google drive][20]]
	- Papers using this dataset
		- Yabin Zhu, Chenglong Li, Yijuan Lu, Liang Lin, Bin Luo, Jin Tang.  
			“FANet : Quality-Aware Feature Aggregation Network for RGB-T Tracking”, 2018 (**PR/SR: 76.4/53.2**)
		- **Xingchen Zhang**, Gang Xiao, Ping Ye, Dan Qiao, Shengyun Peng.  
			  "SiamFT: A Feature-level Fusion Tracking Method Using Fully Convolutional Siamese Networks Based on Visible and Infrared Sequences". ICCV 2019 (submitted).(**PR/SR:65.9/44.8**)
		- **Xingchen Zhang**, Gang Xiao, Ping Ye, Dan Qiao, Junhao Zhao, Shengyun Peng.  
			  "Object Fusion Tracking Based on Visible and Infrared Images Using Fully Convolutional Siamese Networks". Fusion 2019 (Submitted).(**PR/SR:61.0/42.8**)
		- Xingming Zhang, Xuehan Zhang, Xuedan Du, Xiangming Zhou, Jun Yin.  
			"Learning Multi-domain Convolutional Network for RGB-T Visual Tracking." CISP-BMEI 2018.（**PR/SR:61.7/38.7**）
- RIFOT
	- **Xingchen Zhang** et al. BMVC 2019, submitted.

## Results
### GTOT
	Name       | PR   | SR   | Year | Author     |  Type    | FPS |
	FANet      | 88.5 | 69.8 | 2018 | Li et al.  | DL-based | 1.3 |
	SCCF       | 85   | 68.1 | 2018 | Li et al.  | CF-based | 50  |
	LGMG       | 82.9 | 65.5 | 2018 | Li et al.  |          |  7  |
	Cross-modal| 82.7 | 64.3 | 2018 | Li et al.  |          |  8  |
	Fast RGB-T | 77   | 63.2 | 2019 | Zhai et al.| CF-based | 227 |
	Weighted   | 85.12| 62.8 | 2017 | Li et al.  |          |  5  |
	Fusing two | 85.2 | 62.6 | 2018 | Li et al.  | DL-based | 15  |
	Two stage  | 84.2 | 62.2 | 2018 | Li et al.  |          | 7   |
	CSR        | 75   | 62   | 2016 | Li et al.  |          |     |

### RGBT210
	Name       | PR   | SR   | Year | Author     |  Type    | FPS |
	LGMG       | 71.1 | 46.8 | 2018 | Li et al.  |          |  7  |
	Cross-modal| 69.4 | 46.3 | 2018 | Li et al.  |          |  8  | 
	SiamFT     | 65.0 | 44.3 | 2019 |Zhang et al.| DL-bsed  | 25+ |
	Weighted   | 67.5 | 43.0 | 2017 | Li et al.  |          |  5  |  
	Fast RGB-T | 52.9 | 36.6 | 2019 | Zhai et al.| CF-based | 227 |
 
 
### RGBT234
	Name         | PR   | SR   | Year |Author       |  Type    | FPS |
	FANet        | 76.4 | 53.2 | 2018 | Li et al.   | DL-based | 1.3 |
	SGT          | 72.0 | 47.2 | 2018 | Li et al.   |          | 
	SiamFT       | 65.9 | 44.8 | 2019 | Zhang et al.| DL-based | 25+ |
	SiamFC_RGT   | 61.0 | 42.8 | 2019 | Zhang et al.| DL-based |     |  
	Multi-domain | 61.7 | 38.7 | 2018 | Zhang et al.| DL-based |     |


## Distinguished Researchers & Teams
- [Chenglong Li][21], Anhui University, China
- Xiangyuan Lan,Hong Kong Baptist University, Hongkong, China

[2]:	https://www.sciencedirect.com/science/article/pii/S0925231219300347
[3]:	https://ieeexplore.ieee.org/document/8643077
[4]:	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8485393
[5]:	https://www.sciencedirect.com/science/article/pii/S0925231217318271
[6]:	https://www.sciencedirect.com/science/article/pii/S0923596518304892
[7]:	https://link.springer.com/chapter/10.1007%2F978-3-319-27674-8_6
[8]:	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7577747
[9]:	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7577747
[10]:	https://docs.google.com/uc?id=0B-Z6TyBF2ceIZ0c1anVhaHQ3MFk&export=download
[11]:	https://pan.baidu.com/s/1QNidEo-HepRaS6OIZr7-Cw
[12]:	https://www.sciencedirect.com/science/article/pii/S0925231217318271
[13]:	https://www.sciencedirect.com/science/article/pii/S0925231219300347
[14]:	https://dl.acm.org/citation.cfm?id=3123289
[15]:	https://drive.google.com/file/d/0B3i2rdXLNbdUTkhsLVRwcTBTMlU/view
[16]:	https://pan.baidu.com/s/1qXDAq0O#list/path=%2F
[17]:	https://www.sciencedirect.com/science/article/pii/S0925231219300347
[18]:	https://arxiv.org/pdf/1805.08982.pdf
[19]:	https://sites.google.com/view/ahutracking001/
[20]:	(https://drive.google.com/open?id=1ouNEptXOgRop4U7zYMK9zAp57SZ2XCNL)
[21]:	http://cs.ahu.edu.cn/7d/82/c11202a163202/page.htm